{"cells":[{"cell_type":"markdown","metadata":{"id":"EHYqQxpQ6hJE"},"source":[" Attribute Transformation\n","An attribute transform is a function that maps the entire set of values of a given attribute to a new set of replacement values such that each old value can be identified with one of the new values\n","\n","#### Dataset contains features with different metrics and scales. For example --> pregnant and insulin values are based on different scales of measurement. The magnitude of \"insulin\" value is higher than \"pregnant\" in the diabetes dataset. Hence many algorithm that are sensitive to varying scales of value will be biased towards the one with higher magnitdue.For example neural netwroks are highly sensitive to scaling of the data attributes.Hence we need to convert the dataset into suitabe format before it is fed into the neurons.\n","\n","### The solution to varying scale values   \n",">We need a mechanism that scales all the attribute values into a given range typically between 0 to +1 or between a certain specified range. This approach is called feature scaling.\n","\n","> Below are two approaches taht converts each feature into same scale\n","         1. Min-Max Scaler(Normalization)\n","         2. Standardization\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gVKKhofF6hJH"},"source":["Using MinMaxScaler() Rescaling X_train dataset\n","\n","\n","#### minj and maxj represent the minimum and maximum values of attribute j. The jth attribute value $x_{i}^{j}$  of the ith row is scaled as:\n","\n","####                             $y_{i}^{j} = (x_{i}^{j} - min_{j})/(max_{j}-min_{j}) $\n","\n","<font color = red> We transform only the train dataset for scaling or any data tranformation tasks</font>"]},{"cell_type":"markdown","metadata":{"id":"zvFre-Sl6hJI"},"source":["#### split the cleaned data into input  features $(X_{i})$  and output component (Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFpc8Jhe6hJI","outputId":"af7d4371-18f0-4efe-92d7-c33a58b93137"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"]}],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLImNdEt6hJK","outputId":"3f2eab97-5463-464d-ef79-9491ee1da9e4"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pregnant</th>\n","      <th>glucose</th>\n","      <th>bp</th>\n","      <th>skin</th>\n","      <th>insulin</th>\n","      <th>bmi</th>\n","      <th>pedigree</th>\n","      <th>age</th>\n","      <th>Diabetic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>148.0</td>\n","      <td>72</td>\n","      <td>35.00000</td>\n","      <td>125</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>85.0</td>\n","      <td>66</td>\n","      <td>29.00000</td>\n","      <td>125</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8</td>\n","      <td>183.0</td>\n","      <td>64</td>\n","      <td>29.15342</td>\n","      <td>125</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>89.0</td>\n","      <td>66</td>\n","      <td>23.00000</td>\n","      <td>94</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>137.0</td>\n","      <td>40</td>\n","      <td>35.00000</td>\n","      <td>168</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pregnant  glucose  bp      skin  insulin   bmi  pedigree  age  Diabetic\n","0         6    148.0  72  35.00000      125  33.6     0.627   50         1\n","1         1     85.0  66  29.00000      125  26.6     0.351   31         0\n","2         8    183.0  64  29.15342      125  23.3     0.672   32         1\n","3         1     89.0  66  23.00000       94  28.1     0.167   21         0\n","4         0    137.0  40  35.00000      168  43.1     2.288   33         1"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["dbts_new= pd.read_csv('C:/Users/Administrator/dataset/imputed_data_diabetes.csv')\n","dbts_new.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5rEkKty6hJL"},"outputs":[],"source":["spltd_data = dbts_new.values\n","# separate the dataset into input and output components\n","X = spltd_data [:,0:8]\n","Y = spltd_data[:,8]"]},{"cell_type":"markdown","metadata":{"id":"hZ4FvhEG6hJL"},"source":["### Separate the splitted dataset into training and testing dataset with training  dataset = 80% of cleaned data and test dataset  = 20% of cleaned dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rllrcT-g6hJL"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)"]},{"cell_type":"markdown","metadata":{"id":"nr0zlfX66hJM"},"source":["### Use Sci-Kit learn MinMaxScaler () for normlization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s7eYwvRH6hJM","outputId":"e6d3af3f-f575-4f36-cc47-da8108ec7270"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.     0.3742 0.5204 0.1739 0.1334 0.3422 0.2109 0.    ]\n"," [0.     0.6645 0.6224 0.5109 0.1334 0.5971 0.1268 0.0588]\n"," [0.0588 0.4839 0.6327 0.3478 0.2476 0.665  0.3117 0.1569]\n"," [0.0588 0.2968 0.3878 0.1196 0.0541 0.1675 0.5081 0.0784]\n"," [0.     0.471  0.5714 0.2609 0.0469 0.6553 0.0047 0.0588]]\n"]}],"source":["from sklearn.preprocessing import MinMaxScaler\n","sclr = MinMaxScaler(feature_range=(0, 1))\n","scaled_data_X_train = sclr.fit_transform(X_train)\n","# summarize transformed data\n","np.set_printoptions(precision=4)\n","print(scaled_data_X_train[0:5,:])"]},{"cell_type":"markdown","metadata":{"id":"wacBLwM76hJM"},"source":["#### The above code converted all the feature values into the  scale between 0 and 1 using Normalization or Min-Max scaling.\n","<font color = green>Some learning algorithms like Neural Networks expect input values between [0,1] hence we use normalization for scaling in such case. </font>"]},{"cell_type":"markdown","metadata":{"id":"wnHqmie06hJN"},"source":["Standardization\n","****\n","It is another approach to scaling where the scaled value isn't within the [0,1] range. <b>It is suitable where the data collection process has errors and hence has extreme values or outliers.</b>\n","\n","The jth attribute value $x_{i}^{j}$ of the ith row is  normalized by:\n","\n","###                         Z-score_normalization (x')=  ($x_{i}^{j}$ -$\\mu_{j}$)  /  $\\sigma_{j}$\n","\n"," where the $j^{th}$  attribute has mean $\\mu_{j}$ and standard deviation $\\sigma_{j}$ .\n","                       \n","****\n",">We use a function \"StandardScaler()\"  for standardization purpose."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jrri4zbh6hJN","outputId":"eb57189b-833c-4b91-edf0-c560625f89fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[-1.144 -0.662  0.243 -0.714 -0.181 -0.025  0.331 -1.047]\n"," [-1.144  0.81   1.07   2.763 -0.181  1.514 -0.274 -0.793]\n"," [-0.849 -0.106  1.153  1.081  0.918  1.925  1.057 -0.37 ]\n"," [-0.849 -1.054 -0.833 -1.275 -0.944 -1.081  2.471 -0.708]\n"," [-1.144 -0.171  0.656  0.183 -1.013  1.866 -1.153 -0.793]]\n"]}],"source":["from sklearn.preprocessing import StandardScaler\n","scale_ftrs_stndrd = StandardScaler().fit(X_train)\n","scaled_stndrd_X_train = scale_ftrs_stndrd.transform(X_train)\n","# summarize transformed data\n","np.set_printoptions(precision=3)\n","print(scaled_stndrd_X_train[0:5,:])"]},{"cell_type":"markdown","metadata":{"id":"r4ZcdKI46hJN"},"source":["## Dimensionality Reduction\n","#### Dimensionality reduction is all about summarizing the data with most of the information preserved in compact form.Reducing the dimension of the feature space, creates fewer relationships between variables and hence the model is less likely to overfit.\n","\n","#### one of such technique discussed here is the Principal Component Analysis (PCA)\n","****\n","<b> PCA is a  dimensionality-reduction technique for reducing the dimensionality of large data sets, i.e. by transforming a large set of input features into a smaller set which still contains most of the information in the original dataset .But Before applying PCA, the  dataset must be rescaled, if not rescaled, the  model/algorithm's accuracy may not be improved much. </b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dtnw70x56hJO","outputId":"b7cbe93a-9631-4da3-e6f4-0eabdd090270"},"outputs":[{"name":"stdout","output_type":"stream","text":["Explained Variance: [0.28  0.188 0.147]\n"]}],"source":["from sklearn.decomposition import PCA\n","\n","prcpl_cmpnts = PCA(n_components=3)  # use three diagonal compnents for data reduction and summarization\n","prncpl_smmry = prcpl_cmpnts.fit(scaled_stndrd_X_train)\n","print((\"Explained Variance: %s\") % (prncpl_smmry.explained_variance_ratio_)) # summarize the components\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9UXzIyb06hJO","outputId":"3fd89790-5b6f-4e47-e679-96b7f5922849"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 0.291  0.432  0.376  0.401  0.299  0.4    0.128  0.4  ]\n"," [-0.554  0.092 -0.18   0.303  0.199  0.413  0.321 -0.496]\n"," [-0.049  0.443 -0.305 -0.409  0.609 -0.349  0.204  0.082]]\n"]}],"source":["print(prncpl_smmry.components_)"]},{"cell_type":"markdown","metadata":{"id":"KciK_0Wd6hJO"},"source":["****\n","Above code created three principial components as denoted in three separate arrays. Each array represents the component that summarizes the overall data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1cBlOVv6hJO"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}